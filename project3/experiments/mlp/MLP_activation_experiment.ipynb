{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "MLP_activation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLHvWUGW__4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#retirive data from CIFAR-10 dataset\n",
        "def unpickle(fileName):\n",
        "    with open(fileName, 'rb') as f:\n",
        "        dict = pickle.load(f, encoding= \"bytes\")\n",
        "    return dict\n",
        "\n",
        "#merge the bactches of CIFAR, as we have 1 to 5 \n",
        "#load_num represents the number of batches to load\n",
        "def merge_batches(num_to_load=1):\n",
        "    for i in range(1):\n",
        "        fileName = \"data/cifar-10-batches-py/data_batch_\" + str(i + 1)\n",
        "        data = unpickle(fileName)\n",
        "        if i == 0:\n",
        "            features = data[b'data']\n",
        "            labels = np.array(data[b'labels'])\n",
        "        else:\n",
        "            features = np.append(features, data[\"data\"], axis=0)\n",
        "            labels = np.append(labels, data[\"labels\"], axis=0)\n",
        "    return features, labels\n",
        "\n",
        "#one-hot-encode the target label\n",
        "def one_hot_encode(data):\n",
        "    one_hot = np.zeros((data.shape[0], 10))\n",
        "    one_hot[np.arange(data.shape[0]), data] = 1\n",
        "    return one_hot\n",
        "\n",
        "#Normalizing the Pixel Values, input is the list of image pixel values\n",
        "def normalize(data):\n",
        "    return data / 255.0\n",
        "\n",
        "#helper function for the pre_processing, input is the number of batches to load\n",
        "def preprocess(num_to_load=1):\n",
        "    X, y = merge_batches(num_to_load=1)\n",
        "    X = normalize(X)\n",
        "    X = X.reshape(-1, 3072, 1)\n",
        "    y = one_hot_encode(y)\n",
        "    y = y.reshape(-1, 10, 1)\n",
        "    return X, y\n",
        "\n",
        "#splitting the data into training and validation\n",
        "def dataset_split(X, y, ratio=0.8):\n",
        "    split = int(ratio * X.shape[0])\n",
        "    indices = np.random.permutation(X.shape[0])\n",
        "    training_idx, val_idx = indices[:split], indices[split:]\n",
        "    X_train, X_val = X[training_idx, :], X[val_idx, :]\n",
        "    y_train, y_val = y[training_idx, :], y[val_idx, :]\n",
        "    print(\"Records in Training Dataset\", X_train.shape[0])\n",
        "    print(\"Records in Validation Dataset\", X_val.shape[0])\n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "#sigmoid activation\n",
        "def sigmoid(out):\n",
        "    return 1.0 / (1.0 + np.exp(-out))\n",
        "\n",
        "#sigmoid derivative\n",
        "def delta_sigmoid(out):\n",
        "    return sigmoid(out) * (1 - sigmoid(out))\n",
        "#sigmoid cross entropy\n",
        "def SigmoidCrossEntropyLoss(a, y):\n",
        "    return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
        "\n",
        "#the class to define our MLP structure \n",
        "class MLP(object):\n",
        "    #initialize the biases and weights using a Gaussian distribution with mean 0, and variance 1.\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        #setting appropriate dimensions for weights and biases\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    #passing image features to the MLP\n",
        "    def feedforward(self, x):\n",
        "        activation = x\n",
        "        activations = [x]  # list to store activations for every layer\n",
        "        outs = []  # list to store out vectors for every layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            out = np.dot(w, activation) + b\n",
        "            outs.append(out)\n",
        "            activation = sigmoid(out)\n",
        "            activations.append(activation)\n",
        "        return outs, activations\n",
        "\n",
        "    #Data iter to for batching\n",
        "    def get_batch(self, X, y, batch_size):\n",
        "        for batch_idx in range(0, X.shape[0], batch_size):\n",
        "            batch = zip(X[batch_idx:batch_idx + batch_size],\n",
        "                        y[batch_idx:batch_idx + batch_size])\n",
        "            yield batch\n",
        "    \n",
        "    #training phase\n",
        "    def train(self, X, y, X_val, y_val, batch_size=100, learning_rate=0.2, epochs=1000):\n",
        "        n_batches = int(X.shape[0] / batch_size)\n",
        "        acc_array = []\n",
        "        for j in range(epochs):\n",
        "            batch_iter = self.get_batch(X, y, batch_size)\n",
        "            for i in range(n_batches):\n",
        "                batch = next(batch_iter)\n",
        "                # same shape as self.biases\n",
        "                del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "                # same shape as self.weights\n",
        "                del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "                for batch_X, batch_y in batch:\n",
        "                    # accumulate all the bias and weight gradients\n",
        "                    loss, delta_del_b, delta_del_w = self.backpropagate(\n",
        "                        batch_X, batch_y)\n",
        "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
        "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
        "            accuracy = self.eval(X_val, y_val)\n",
        "            self.weights = [w - (learning_rate / batch_size)\n",
        "                            * delw for w, delw in zip(self.weights, del_w)]\n",
        "            self.biases = [b - (learning_rate / batch_size)\n",
        "                           * delb for b, delb in zip(self.biases, del_b)]\n",
        "            print(\"\\nEpoch %d complete\\tLoss: %f\\n\" % (j, loss))\n",
        "            acc_array.append(accuracy)\n",
        "        return acc_array\n",
        "\n",
        "    def backpropagate(self, x, y):\n",
        "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        outs, activations = self.feedforward(x)\n",
        "        loss = SigmoidCrossEntropyLoss(activations[-1], y) #cost function\n",
        "        # calculate derivative of cost Sigmoid Cross entropy which is to be minimized\n",
        "        delta_cost = activations[-1] - y\n",
        "        # backward pass to reduce cost\n",
        "        # gradients at output layers\n",
        "        delta = delta_cost\n",
        "        del_b[-1] = delta\n",
        "        del_w[-1] = np.dot(delta, activations[-2].T)\n",
        "\n",
        "        # updating gradients of each layer using reverse or negative indexing\n",
        "        for l in range(2, self.num_layers):\n",
        "            out = outs[-l]\n",
        "            delta_activation = delta_sigmoid(out)\n",
        "            delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation\n",
        "            del_b[-l] = delta\n",
        "            del_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
        "        return (loss, del_b, del_w)\n",
        "\n",
        "    #Evaluation Phase\n",
        "    def eval(self, X, y):\n",
        "        count = 0\n",
        "        for x, _y in zip(X, y):\n",
        "            outs, activations = self.feedforward(x)\n",
        "            # postion of maximum value is the predicted label\n",
        "            if np.argmax(activations[-1]) == np.argmax(_y):\n",
        "                count += 1\n",
        "        print(\"Accuracy: %f\" % ((float(count) / X.shape[0]) * 100))\n",
        "        return ((float(count) / X.shape[0]) * 100)\n",
        "\n",
        "    def predict(self, X):\n",
        "        labels = unpickle(\"data/cifar-10-batches-py/batches.meta\")[b\"label_names\"]\n",
        "        preds = np.array([])\n",
        "        for x in X:\n",
        "            outs, activations = self.feedforward(x)\n",
        "            preds = np.append(preds, np.argmax(activations[-1]))\n",
        "        preds = np.array([labels[int(p)] for p in preds])\n",
        "        return preds\n",
        "\n",
        "\n",
        "def sigmoid():\n",
        "    X, y = preprocess(num_to_load=1)\n",
        "    X_train, y_train, X_val, y_val = dataset_split(X, y)\n",
        "    model = DNN([3072, 50, 30, 10])  # initialize the model\n",
        "    acc_array = model.train(X_train, y_train, X_val, y_val, epochs=100)  # train the model\n",
        "    model.eval(X_val, y_val)  # check accuracy using validation set\n",
        "    # preprocess test dataset\n",
        "    test_X = unpickle(\"data/cifar-10-batches-py/test_batch\")[b'data'] / 255.0\n",
        "    test_X = test_X.reshape(-1, 3072, 1)\n",
        "    # make predictions of test dataset\n",
        "    print(model.predict(test_X))\n",
        "    return acc_array\n",
        "\n",
        "\n",
        "#the accuracy array\n",
        "acc_array = sigmoid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cacsH6eX__4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#retirive data from CIFAR-10 dataset\n",
        "def unpickle(fileName):\n",
        "    with open(fileName, 'rb') as f:\n",
        "        dict = pickle.load(f, encoding= \"bytes\")\n",
        "    return dict\n",
        "\n",
        "#merge the bactches of CIFAR, as we have 1 to 5 \n",
        "#load_num represents the number of batches to load\n",
        "def merge_batches(num_to_load=1):\n",
        "    for i in range(1):\n",
        "        fileName = \"data/cifar-10-batches-py/data_batch_\" + str(i + 1)\n",
        "        data = unpickle(fileName)\n",
        "        if i == 0:\n",
        "            features = data[b'data']\n",
        "            labels = np.array(data[b'labels'])\n",
        "        else:\n",
        "            features = np.append(features, data[\"data\"], axis=0)\n",
        "            labels = np.append(labels, data[\"labels\"], axis=0)\n",
        "    return features, labels\n",
        "\n",
        "#one-hot-encode the target label\n",
        "def one_hot_encode(data):\n",
        "    one_hot = np.zeros((data.shape[0], 10))\n",
        "    one_hot[np.arange(data.shape[0]), data] = 1\n",
        "    return one_hot\n",
        "\n",
        "#Normalizing the Pixel Values, input is the list of image pixel values\n",
        "def normalize(data):\n",
        "    return data / 255.0\n",
        "\n",
        "#helper function for the pre_processing, input is the number of batches to load\n",
        "def preprocess(num_to_load=1):\n",
        "    X, y = merge_batches(num_to_load=1)\n",
        "    X = normalize(X)\n",
        "    X = X.reshape(-1, 3072, 1)\n",
        "    y = one_hot_encode(y)\n",
        "    y = y.reshape(-1, 10, 1)\n",
        "    return X, y\n",
        "\n",
        "#splitting the data into training and validation\n",
        "def dataset_split(X, y, ratio=0.8):\n",
        "    split = int(ratio * X.shape[0])\n",
        "    indices = np.random.permutation(X.shape[0])\n",
        "    training_idx, val_idx = indices[:split], indices[split:]\n",
        "    X_train, X_val = X[training_idx, :], X[val_idx, :]\n",
        "    y_train, y_val = y[training_idx, :], y[val_idx, :]\n",
        "    print(\"Records in Training Dataset\", X_train.shape[0])\n",
        "    print(\"Records in Validation Dataset\", X_val.shape[0])\n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "#activation\n",
        "def logistic(z):\n",
        "    pos_num = np.where(z >= 0)\\n\n",
        "    neg_num = np.where(z < 0)\\n\n",
        "    ans = np.empty(z.shape)\\n\n",
        "    ans[pos_num[0], pos_num[1]] = 1 / (1 + np.exp(-z[pos_num[0], pos_num[1]]))\\n\n",
        "    ans[neg_num[0], neg_num[1]] = np.exp(z[neg_num[0], neg_num[1]]) / (1 + np.exp(z[neg_num[0], neg_num[1]]))\\n\n",
        "    return ans\n",
        "\n",
        "#derivative\n",
        "def delta_logistic(out):\n",
        "    return logistic(out) * (1 - logistic(out))\n",
        "# cross entropy\n",
        "def CrossEntropyLoss(a, y):\n",
        "    return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
        "\n",
        "#the class to define our MLP structure \n",
        "class MLP(object):\n",
        "    #initialize the biases and weights using a Gaussian distribution with mean 0, and variance 1.\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        #setting appropriate dimensions for weights and biases\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    #passing image features to the MLP\n",
        "    def feedforward(self, x):\n",
        "        activation = x\n",
        "        activations = [x]  # list to store activations for every layer\n",
        "        outs = []  # list to store out vectors for every layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            out = np.dot(w, activation) + b\n",
        "            outs.append(out)\n",
        "            activation = (out)\n",
        "            activations.append(activation)\n",
        "        return outs, activations\n",
        "\n",
        "    #Data iter to for batching\n",
        "    def get_batch(self, X, y, batch_size):\n",
        "        for batch_idx in range(0, X.shape[0], batch_size):\n",
        "            batch = zip(X[batch_idx:batch_idx + batch_size],\n",
        "                        y[batch_idx:batch_idx + batch_size])\n",
        "            yield batch\n",
        "    \n",
        "    #training phase\n",
        "    def train(self, X, y, X_val, y_val, batch_size=100, learning_rate=0.2, epochs=1000):\n",
        "        n_batches = int(X.shape[0] / batch_size)\n",
        "        acc_array = []\n",
        "        for j in range(epochs):\n",
        "            batch_iter = self.get_batch(X, y, batch_size)\n",
        "            for i in range(n_batches):\n",
        "                batch = next(batch_iter)\n",
        "                # same shape as self.biases\n",
        "                del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "                # same shape as self.weights\n",
        "                del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "                for batch_X, batch_y in batch:\n",
        "                    # accumulate all the bias and weight gradients\n",
        "                    loss, delta_del_b, delta_del_w = self.backpropagate(\n",
        "                        batch_X, batch_y)\n",
        "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
        "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
        "            accuracy = self.eval(X_val, y_val)\n",
        "            self.weights = [w - (learning_rate / batch_size)\n",
        "                            * delw for w, delw in zip(self.weights, del_w)]\n",
        "            self.biases = [b - (learning_rate / batch_size)\n",
        "                           * delb for b, delb in zip(self.biases, del_b)]\n",
        "            print(\"\\nEpoch %d complete\\tLoss: %f\\n\" % (j, loss))\n",
        "            acc_array.append(accuracy)\n",
        "        return acc_array\n",
        "\n",
        "    def backpropagate(self, x, y):\n",
        "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        outs, activations = self.feedforward(x)\n",
        "        loss = CrossEntropyLoss(activations[-1], y) #cost function\n",
        "        # calculate derivative of cost Cross entropy which is to be minimized\n",
        "        delta_cost = activations[-1] - y\n",
        "        # backward pass to reduce cost\n",
        "        # gradients at output layers\n",
        "        delta = delta_cost\n",
        "        del_b[-1] = delta\n",
        "        del_w[-1] = np.dot(delta, activations[-2].T)\n",
        "\n",
        "        # updating gradients of each layer using reverse or negative indexing\n",
        "        for l in range(2, self.num_layers):\n",
        "            out = outs[-l]\n",
        "            delta_activation = delta_logistic(out)\n",
        "            delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation\n",
        "            del_b[-l] = delta\n",
        "            del_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
        "        return (loss, del_b, del_w)\n",
        "\n",
        "    #Evaluation Phase\n",
        "    def eval(self, X, y):\n",
        "        count = 0\n",
        "        for x, _y in zip(X, y):\n",
        "            outs, activations = self.feedforward(x)\n",
        "            # postion of maximum value is the predicted label\n",
        "            if np.argmax(activations[-1]) == np.argmax(_y):\n",
        "                count += 1\n",
        "        print(\"Accuracy: %f\" % ((float(count) / X.shape[0]) * 100))\n",
        "        return ((float(count) / X.shape[0]) * 100)\n",
        "\n",
        "    def predict(self, X):\n",
        "        labels = unpickle(\"data/cifar-10-batches-py/batches.meta\")[b\"label_names\"]\n",
        "        preds = np.array([])\n",
        "        for x in X:\n",
        "            outs, activations = self.feedforward(x)\n",
        "            preds = np.append(preds, np.argmax(activations[-1]))\n",
        "        preds = np.array([labels[int(p)] for p in preds])\n",
        "        return preds\n",
        "\n",
        "def logistic():\n",
        "    X, y = preprocess(num_to_load=1)\n",
        "    X_train, y_train, X_val, y_val = dataset_split(X, y)\n",
        "    model = DNN([3072, 50, 30 10])  # initialize the model\n",
        "    acc_array2 = model.train(X_train, y_train, X_val, y_val, epochs=100)  # train the model\n",
        "    model.eval(X_val, y_val)  # check accuracy using validation set\n",
        "    # preprocess test dataset\n",
        "    test_X = unpickle(\"data/cifar-10-batches-py/test_batch\")[b'data'] / 255.0\n",
        "    test_X = test_X.reshape(-1, 3072, 1)\n",
        "    # make predictions of test dataset\n",
        "    print(model.predict(test_X))\n",
        "    return acc_array2\n",
        "\n",
        "\n",
        "#the accuracy array\n",
        "acc_array2 = logistic()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoCp47BH__4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#retirive data from CIFAR-10 dataset\n",
        "def unpickle(fileName):\n",
        "    with open(fileName, 'rb') as f:\n",
        "        dict = pickle.load(f, encoding= \"bytes\")\n",
        "    return dict\n",
        "\n",
        "#merge the bactches of CIFAR, as we have 1 to 5 \n",
        "#load_num represents the number of batches to load\n",
        "def merge_batches(num_to_load=1):\n",
        "    for i in range(1):\n",
        "        fileName = \"data/cifar-10-batches-py/data_batch_\" + str(i + 1)\n",
        "        data = unpickle(fileName)\n",
        "        if i == 0:\n",
        "            features = data[b'data']\n",
        "            labels = np.array(data[b'labels'])\n",
        "        else:\n",
        "            features = np.append(features, data[\"data\"], axis=0)\n",
        "            labels = np.append(labels, data[\"labels\"], axis=0)\n",
        "    return features, labels\n",
        "\n",
        "#one-hot-encode the target label\n",
        "def one_hot_encode(data):\n",
        "    one_hot = np.zeros((data.shape[0], 10))\n",
        "    one_hot[np.arange(data.shape[0]), data] = 1\n",
        "    return one_hot\n",
        "\n",
        "#Normalizing the Pixel Values, input is the list of image pixel values\n",
        "def normalize(data):\n",
        "    return data / 255.0\n",
        "\n",
        "#helper function for the pre_processing, input is the number of batches to load\n",
        "def preprocess(num_to_load=1):\n",
        "    X, y = merge_batches(num_to_load=1)\n",
        "    X = normalize(X)\n",
        "    X = X.reshape(-1, 3072, 1)\n",
        "    y = one_hot_encode(y)\n",
        "    y = y.reshape(-1, 10, 1)\n",
        "    return X, y\n",
        "\n",
        "#splitting the data into training and validation\n",
        "def dataset_split(X, y, ratio=0.8):\n",
        "    split = int(ratio * X.shape[0])\n",
        "    indices = np.random.permutation(X.shape[0])\n",
        "    training_idx, val_idx = indices[:split], indices[split:]\n",
        "    X_train, X_val = X[training_idx, :], X[val_idx, :]\n",
        "    y_train, y_val = y[training_idx, :], y[val_idx, :]\n",
        "    print(\"Records in Training Dataset\", X_train.shape[0])\n",
        "    print(\"Records in Validation Dataset\", X_val.shape[0])\n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "#activation\n",
        "def reLU(z):\n",
        "    u = np.ones(z.shape)\\n\n",
        "    u[z < 0] = np.double(0.01)\\n\n",
        "    z = z * u\\n\n",
        "    return z\\n\n",
        "\n",
        "#derivative\n",
        "def delta_reLU(out):\n",
        "    return reLU(out) * (1 - reLU(out))\n",
        "#sigmoid cross entropy\n",
        "def SigmoidCrossEntropyLoss(a, y):\n",
        "    return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
        "\n",
        "#the class to define our MLP structure \n",
        "class MLP(object):\n",
        "    #initialize the biases and weights using a Gaussian distribution with mean 0, and variance 1.\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        #setting appropriate dimensions for weights and biases\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    #passing image features to the MLP\n",
        "    def feedforward(self, x):\n",
        "        activation = x\n",
        "        activations = [x]  # list to store activations for every layer\n",
        "        outs = []  # list to store out vectors for every layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            out = np.dot(w, activation) + b\n",
        "            outs.append(out)\n",
        "            activation = sigmoid(out)\n",
        "            activations.append(activation)\n",
        "        return outs, activations\n",
        "\n",
        "    #Data iter to for batching\n",
        "    def get_batch(self, X, y, batch_size):\n",
        "        for batch_idx in range(0, X.shape[0], batch_size):\n",
        "            batch = zip(X[batch_idx:batch_idx + batch_size],\n",
        "                        y[batch_idx:batch_idx + batch_size])\n",
        "            yield batch\n",
        "    \n",
        "    #training phase\n",
        "    def train(self, X, y, X_val, y_val, batch_size=100, learning_rate=0.2, epochs=1000):\n",
        "        n_batches = int(X.shape[0] / batch_size)\n",
        "        acc_array = []\n",
        "        for j in range(epochs):\n",
        "            batch_iter = self.get_batch(X, y, batch_size)\n",
        "            for i in range(n_batches):\n",
        "                batch = next(batch_iter)\n",
        "                # same shape as self.biases\n",
        "                del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "                # same shape as self.weights\n",
        "                del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "                for batch_X, batch_y in batch:\n",
        "                    # accumulate all the bias and weight gradients\n",
        "                    loss, delta_del_b, delta_del_w = self.backpropagate(\n",
        "                        batch_X, batch_y)\n",
        "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
        "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
        "            accuracy = self.eval(X_val, y_val)\n",
        "            self.weights = [w - (learning_rate / batch_size)\n",
        "                            * delw for w, delw in zip(self.weights, del_w)]\n",
        "            self.biases = [b - (learning_rate / batch_size)\n",
        "                           * delb for b, delb in zip(self.biases, del_b)]\n",
        "            print(\"\\nEpoch %d complete\\tLoss: %f\\n\" % (j, loss))\n",
        "            acc_array.append(accuracy)\n",
        "        return acc_array\n",
        "\n",
        "    def backpropagate(self, x, y):\n",
        "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        outs, activations = self.feedforward(x)\n",
        "        loss = SigmoidCrossEntropyLoss(activations[-1], y) #cost function\n",
        "        # calculate derivative of cost Sigmoid Cross entropy which is to be minimized\n",
        "        delta_cost = activations[-1] - y\n",
        "        # backward pass to reduce cost\n",
        "        # gradients at output layers\n",
        "        delta = delta_cost\n",
        "        del_b[-1] = delta\n",
        "        del_w[-1] = np.dot(delta, activations[-2].T)\n",
        "\n",
        "        # updating gradients of each layer using reverse or negative indexing\n",
        "        for l in range(2, self.num_layers):\n",
        "            out = outs[-l]\n",
        "            delta_activation = delta_sigmoid(out)\n",
        "            delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation\n",
        "            del_b[-l] = delta\n",
        "            del_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
        "        return (loss, del_b, del_w)\n",
        "\n",
        "    #Evaluation Phase\n",
        "    def eval(self, X, y):\n",
        "        count = 0\n",
        "        for x, _y in zip(X, y):\n",
        "            outs, activations = self.feedforward(x)\n",
        "            # postion of maximum value is the predicted label\n",
        "            if np.argmax(activations[-1]) == np.argmax(_y):\n",
        "                count += 1\n",
        "        print(\"Accuracy: %f\" % ((float(count) / X.shape[0]) * 100))\n",
        "        return ((float(count) / X.shape[0]) * 100)\n",
        "\n",
        "    def predict(self, X):\n",
        "        labels = unpickle(\"data/cifar-10-batches-py/batches.meta\")[b\"label_names\"]\n",
        "        preds = np.array([])\n",
        "        for x in X:\n",
        "            outs, activations = self.feedforward(x)\n",
        "            preds = np.append(preds, np.argmax(activations[-1]))\n",
        "        preds = np.array([labels[int(p)] for p in preds])\n",
        "        return preds\n",
        "\n",
        "def relu():\n",
        "    X, y = preprocess(num_to_load=1)\n",
        "    X_train, y_train, X_val, y_val = dataset_split(X, y)\n",
        "    model = DNN([3072, 50, 30 10])  # initialize the model\n",
        "    acc_array3 = model.train(X_train, y_train, X_val, y_val, epochs=100)  # train the model\n",
        "    model.eval(X_val, y_val)  # check accuracy using validation set\n",
        "    # preprocess test dataset\n",
        "    test_X = unpickle(\"data/cifar-10-batches-py/test_batch\")[b'data'] / 255.0\n",
        "    test_X = test_X.reshape(-1, 3072, 1)\n",
        "    # make predictions of test dataset\n",
        "    print(model.predict(test_X))\n",
        "    return acc_array3\n",
        "\n",
        "\n",
        "#the accuracy array\n",
        "acc_array3 = relu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH6FlIkKJLAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot(acc_array, 'r', label='Test accuracy of MLP with sigmoid activation')\n",
        "plt.plot(acc_array2, 'g', label='Test accuracy of MLP with logistic activation')\n",
        "plt.plot(acc_array3, 'y', label='Test accuracy of MLP with relu activation')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}